{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import os, sys, email,re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_summary</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tableau, visualisierung</td>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, implementierung, java, ms sql, camel, apache...</td>\n",
       "      <td>Developer - Talend ESB oder Apache Camel (m/w)...</td>\n",
       "      <td>Konzeption, Customizing sowie Softwareanpassun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, windows, clients, ios, mobile devices, mobil...</td>\n",
       "      <td>IT-Mitarbeiter (m/w) 1st / 2nd Level Support /...</td>\n",
       "      <td>Als Mitglied eines kleinen, dynamischen Teams ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dynamics ax 2009, dynamics crm 4.0, dynamics a...</td>\n",
       "      <td>Consultant für Microsoft Dynamics CRM (m/w)</td>\n",
       "      <td>Über unseren Kunden: \\n   \\n Unser Kunde ist e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hibernate, java, deutsch, oracle 11g, oracle 1...</td>\n",
       "      <td>Java Senior Developer - Backend (f/m)</td>\n",
       "      <td>Über unseren Kunden: \\n   \\n Unser Kunde ist e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       skill_summary  \\\n",
       "0                            tableau, visualisierung   \n",
       "1  , implementierung, java, ms sql, camel, apache...   \n",
       "2  , windows, clients, ios, mobile devices, mobil...   \n",
       "3  dynamics ax 2009, dynamics crm 4.0, dynamics a...   \n",
       "4  hibernate, java, deutsch, oracle 11g, oracle 1...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Business Intelligence Analyst (m/w) - Tableau ...   \n",
       "1  Developer - Talend ESB oder Apache Camel (m/w)...   \n",
       "2  IT-Mitarbeiter (m/w) 1st / 2nd Level Support /...   \n",
       "3        Consultant für Microsoft Dynamics CRM (m/w)   \n",
       "4              Java Senior Developer - Backend (f/m)   \n",
       "\n",
       "                                         description  \n",
       "0  Business Intelligence Analyst (m/w) - Tableau ...  \n",
       "1  Konzeption, Customizing sowie Softwareanpassun...  \n",
       "2  Als Mitglied eines kleinen, dynamischen Teams ...  \n",
       "3  Über unseren Kunden: \\n   \\n Unser Kunde ist e...  \n",
       "4  Über unseren Kunden: \\n   \\n Unser Kunde ist e...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../projectfinder.csv')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.head()\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split features\n",
    "title = df['title'].tolist()\n",
    "description = df['description'].tolist()\n",
    "skill = df['skill_summary'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine to make a lists of lists\n",
    "all_feat = [title, description, skill]\n",
    "features = ['title', 'description', 'skill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "      <td>tableau, visualisierung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Developer - Talend ESB oder Apache Camel (m/w)...</td>\n",
       "      <td>Konzeption, Customizing sowie Softwareanpassun...</td>\n",
       "      <td>, implementierung, java, ms sql, camel, apache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT-Mitarbeiter (m/w) 1st / 2nd Level Support /...</td>\n",
       "      <td>Als Mitglied eines kleinen, dynamischen Teams ...</td>\n",
       "      <td>, windows, clients, ios, mobile devices, mobil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consultant für Microsoft Dynamics CRM (m/w)</td>\n",
       "      <td>Über unseren Kunden: \\n   \\n Unser Kunde ist e...</td>\n",
       "      <td>dynamics ax 2009, dynamics crm 4.0, dynamics a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Java Senior Developer - Backend (f/m)</td>\n",
       "      <td>Über unseren Kunden: \\n   \\n Unser Kunde ist e...</td>\n",
       "      <td>hibernate, java, deutsch, oracle 11g, oracle 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Business Intelligence Analyst (m/w) - Tableau ...   \n",
       "1  Developer - Talend ESB oder Apache Camel (m/w)...   \n",
       "2  IT-Mitarbeiter (m/w) 1st / 2nd Level Support /...   \n",
       "3        Consultant für Microsoft Dynamics CRM (m/w)   \n",
       "4              Java Senior Developer - Backend (f/m)   \n",
       "\n",
       "                                         description  \\\n",
       "0  Business Intelligence Analyst (m/w) - Tableau ...   \n",
       "1  Konzeption, Customizing sowie Softwareanpassun...   \n",
       "2  Als Mitglied eines kleinen, dynamischen Teams ...   \n",
       "3  Über unseren Kunden: \\n   \\n Unser Kunde ist e...   \n",
       "4  Über unseren Kunden: \\n   \\n Unser Kunde ist e...   \n",
       "\n",
       "                                               skill  \n",
       "0                            tableau, visualisierung  \n",
       "1  , implementierung, java, ms sql, camel, apache...  \n",
       "2  , windows, clients, ios, mobile devices, mobil...  \n",
       "3  dynamics ax 2009, dynamics crm 4.0, dynamics a...  \n",
       "4  hibernate, java, deutsch, oracle 11g, oracle 1...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new dataframe\n",
    "new_df = pd.DataFrame(all_feat).transpose()\n",
    "new_df.columns = features\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n",
      "805\n"
     ]
    }
   ],
   "source": [
    "# load nltk's German stopwords'\n",
    "with open('../stopwords-de.txt', 'r') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "print(len(stopwords))\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(stopwords_eng)\n",
    "stopwords.extend(['bewerben' 'direkt', 'melden', 'www', 'contactor'])\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# load nltk's SnowballStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isalph(mylist):\n",
    "    return [item for item in mylist if item.isalpha()]\n",
    "def remove_stop_words(mylist):\n",
    "    return [item for item in mylist if item not in stopwords]\n",
    "def lowercase(mylist):\n",
    "    return [item.lower() for item in mylist]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_list = [isalph(sub) for sub in list_of_tokens ]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# text_to_string1 = ','.join(str(v) for v in all_feat)\n",
    "#text_to_string1[0]\n",
    "test = [['I miss having someone to talk to all night..'], ['Pergunte-me qualquer coisa'], ['RT @Caracolinhos13: Tenho a tl cheia dessa merda de quem vos visitou nas \\\\xc3\\\\xbaltimas horas'], ['RT @B24pt: #CarlosHadADream'], ['Tudo tem um fim'], ['RT @thechgama: stalkear as curtidas \\\\xc3\\\\xa9 um caminho sem volta'], ['Como consegues fumar 3 purexs seguidas? \\\\xe2\\\\x80\\\\x94 Eram 2 purex e mix...']]\n",
    "flat_list = [item for sublist in test for item in sublist]\n",
    "list_of_tokens1 = [nltk.word_tokenize(sentence) for sentence in flat_list ]\n",
    "list_lower1 = [lowercase(word) for word in list_of_tokens1]\n",
    "#list_of_tokens\n",
    "new_list1 = [isalph(sub) for sub in list_lower1 ]\n",
    "cleaned1 = [remove_stop_words(word) for word in new_list1 ]\n",
    "print(cleaned1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    #convert text list from list to a string\n",
    "    #text_to_string = ','.join(str(v) for v in text)\n",
    "    flatten_list = [item for sublist in text for item in sublist]\n",
    "    # sentence tokenization and lowercasing first, then by word to ensure that punctuation is caught as it's own token\n",
    "    list_of_tokens = [(nltk.word_tokenize(word)) for word in flatten_list]\n",
    "    \n",
    "    #extract only alphabetic tokens\n",
    "    alphanumeric_tokens = [isalph(sub) for sub in list_of_tokens ]\n",
    "    print(alphanumeric_tokens[:5])\n",
    "    \n",
    "    #lower case\n",
    "    lowercased_list = [lowercase(word) for word in alphanumeric_tokens]\n",
    "    #stopwords removal\n",
    "    cleaned = [remove_stop_words(word) for word in lowercased_list]\n",
    "    print(cleaned[:20])\n",
    "    \n",
    "    #stemming \n",
    "    #double for loop because its a lits of lits\n",
    "    stemmed_tokens = [stemmer.stem(item) for token in cleaned for item in token]\n",
    "    print(stemmed_tokens[:20])\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "# this is used only for presentational purposes\n",
    "def tokenize_only(text):\n",
    "    #convert text list from list to a string\n",
    "    #text_to_string = ','.join(str(v) for v in text)\n",
    "    flatten_list = [item for sublist in text for item in sublist]\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    list_of_tokens = [(nltk.word_tokenize(word)) for word in flatten_list]\n",
    "    #lower case\n",
    "    lowercased_list = [lowercase(word) for word in list_of_tokens]\n",
    "    #stopwords removal\n",
    "    cleaned = [remove_stop_words(word) for word in lowercased_list]\n",
    "    print(cleaned[:20])\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    filtered_tokens = [isalph(sub) for sub in cleaned ]\n",
    "    \n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Business', 'Intelligence', 'Analyst', 'Tableau', 'Desktop', 'FFM'], ['Developer', 'Talend', 'ESB', 'oder', 'Apache', 'Camel'], ['Level', 'Support', 'Onsite', 'Support'], ['Consultant', 'für', 'Microsoft', 'Dynamics', 'CRM'], ['Java', 'Senior', 'Developer', 'Backend']]\n",
      "[['business', 'intelligence', 'analyst', 'tableau', 'desktop', 'ffm'], ['developer', 'talend', 'esb', 'apache', 'camel'], ['level', 'support', 'onsite', 'support'], ['consultant', 'microsoft', 'dynamics', 'crm'], ['java', 'senior', 'developer', 'backend'], ['dms', 'cms', 'consultant', 'prozeßerfahrung'], ['digitalisierung', 'galvanik', 'endkunde'], ['ux', 'designer', 'web', 'app'], ['linux', 'systemadministrator'], ['requirements', 'engineer', 'wertpapiere'], ['developer'], ['java', 'entwickler'], ['system', 'engineer', 'schwerpunkt', 'ms', 'exchange', 'active', 'directory'], ['sap', 'technical', 'consultant', 'sap', 'entwickler'], ['inhouse', 'prozessmanager', 'dynamics', 'nav'], ['business', 'intelligence', 'consultant'], ['java', 'software', 'entwickler'], ['system', 'engineer', 'microsoft', 'sharepoint'], ['sap', 'consultant'], ['sap', 'senior', 'consultant', 'business', 'intelligence']]\n",
      "['business', 'intelligenc', 'analyst', 'tableau', 'desktop', 'ffm', 'develop', 'talend', 'esb', 'apach', 'camel', 'level', 'support', 'onsit', 'support', 'consultant', 'microsoft', 'dynamics', 'crm', 'java']\n",
      "[['business', 'intelligence', 'analyst', '(', 'm/w', ')', '-', 'tableau', 'desktop', '-', 'ffm'], ['developer', '-', 'talend', 'esb', 'apache', 'camel', '(', 'm/w', ')', '–', 'schleswig-holstein', '#', '3076'], ['it-mitarbeiter', '(', 'm/w', ')', '1st', '/', '2nd', 'level', 'support', '/', 'onsite', 'support', '#', '3601'], ['consultant', 'microsoft', 'dynamics', 'crm', '(', 'm/w', ')'], ['java', 'senior', 'developer', '-', 'backend', '(', 'f/m', ')'], ['dms', '-', 'cms', 'consultant', 'prozeßerfahrung', '(', 'm/w', ')'], ['digitalisierung', 'galvanik', 'endkunde'], ['ux', 'designer', 'web', 'app', '(', 'm/w', ')'], ['linux', 'systemadministrator', '(', 'm/w', ')'], ['requirements', 'engineer', '-', 'wertpapiere', '(', 'm/w', ')'], ['c++', 'developer'], ['java', 'entwickler', '(', 'm/w', ')'], ['system', 'engineer', '(', 'm/w', ')', 'schwerpunkt', 'ms', 'exchange', 'active', 'directory'], ['sap', 'technical', 'consultant', '/', 'sap', 'entwickler', '(', 'm/w', ')'], ['inhouse', 'prozessmanager', '(', 'm/w', ')', 'dynamics', 'nav'], ['business', 'intelligence', 'consultant', '(', 'm/w', ')'], ['java', 'software', 'entwickler', '(', 'm/w', ')'], ['system', 'engineer', '(', 'm/w', ')', 'microsoft', 'sharepoint'], ['sap', 'pi/po', 'consultant', '(', 'm/w', ')'], ['sap', 'senior', 'consultant', 'business', 'intelligence', '(', 'm/w', ')']]\n"
     ]
    }
   ],
   "source": [
    "stemmed_text = tokenize_and_stem(all_feat)\n",
    "tokenized_text = tokenize_only(all_feat)\n",
    "#for word in description:\n",
    " #   stemmed_word = tokenize_and_stem(word) #for each item in 'synopses', tokenize/stem\n",
    "  #  all_tokenized = tokenize_only(word)\n",
    "   # stemmed_text.extend(stemmed_word) #extend the 'totalvocab_stemmed' list\n",
    "    #tokenized_text.extend(all_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_2 = df['title'] + ' '  + df['description'] + ' '  + df['skill_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 5\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer(ngram_range=(1, 3), stop_words=stopwords)),\n",
    "    ('clusterer',  KMeans(n_clusters=true_k)) ])\n",
    "\n",
    "pipeline.fit(combined_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer(ngram_range=(1, 3), stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wahr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5360, 603175)\n"
     ]
    }
   ],
   "source": [
    "X = vector.fit_transform(combined_2)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "sap\n",
      "bewerben\n",
      "direkt\n",
      "melden\n",
      "bewerben direkt\n",
      "direkt bewerben\n",
      "melden bewerben\n",
      "bewerben direkt bewerben\n",
      "melden bewerben direkt\n",
      "abap\n",
      "\n",
      "Cluster 1\n",
      "bewerben\n",
      "direkt\n",
      "melden\n",
      "direkt bewerben\n",
      "bewerben direkt\n",
      "melden bewerben direkt\n",
      "melden bewerben\n",
      "bewerben direkt bewerben\n",
      "java\n",
      "kenntnisse\n",
      "\n",
      "Cluster 2\n",
      "contractor\n",
      "bewerben\n",
      "https\n",
      "frankfurt\n",
      "https www contractor\n",
      "https www\n",
      "contractor de\n",
      "de\n",
      "www contractor de\n",
      "www contractor\n",
      "\n",
      "Cluster 3\n",
      "contractor\n",
      "https\n",
      "bewerben\n",
      "münchen\n",
      "www contractor de\n",
      "www\n",
      "https www contractor\n",
      "https www\n",
      "www contractor\n",
      "contractor de\n",
      "\n",
      "Cluster 4\n",
      "bewerben\n",
      "daten\n",
      "cv\n",
      "bewerben direkt bewerben\n",
      "direkt\n",
      "direkt bewerben\n",
      "melden bewerben\n",
      "bewerben direkt\n",
      "melden bewerben direkt\n",
      "melden\n"
     ]
    }
   ],
   "source": [
    "for i in range(true_k):\n",
    "    print()\n",
    "    print(f'Cluster {i}')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prediction\")\n",
    "X = vectorizer.transform([\"Produktumfeld der Firma VMWARE: Airwatch (sehr gute Kenntnisse)Netzwerktechnik: LAN, DMZ, Rechenzentrum (Vertiefte Kenntnisse) MS-Office Tools: Word, Excel, Visio, Powerpoint (Vertiefte Kenntnisse) Bereitschaft zur Sicherheitsüberprüfung Level 2 (Ü2)\"])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [['Konzeption, Customizing sowie Softwareanpassungen mit Talend ESB  Implementierung von Softwaresystemen mit Java  Analyse sowie Design von Softwarearchitekturen'], ['Developer - Talend ESB oder Apache Camel (m/w) – Schleswig-Holstein #3076'],[\"mybatis\",\"datenbanken\",\"analyse\",\"talend\",\"softwarearchitektur\",\"java\",\"sql\",\"datenmodelle\",\"ms sql\",\"requirements\",\"apache camel\",\"testautomatisierung\",\"ms\",\"webservices\",\"oracle\",\"entwicklung\",\"esb\",\"apache\",\"camel\",\"implementierung\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-554735d8734d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.transform(test_data)\n",
    "predicted = model.predict(vec)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
